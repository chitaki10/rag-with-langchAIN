{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAvf5ngxVkqB"
      },
      "source": [
        "# üîß Step 1: Install Ollama\n",
        "\n",
        "**What is Ollama?**\n",
        "- Local embedding model server (runs on your machine)\n",
        "- Converts text into 768-dimensional vectors\n",
        "- Completely free, no API costs\n",
        "\n",
        "**What this cell does:**\n",
        "- Installs Ollama software (~2 minutes)\n",
        "- Detects GPU if available\n",
        "- Sets up the server infrastructure\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwE4ui3sXga8",
        "outputId": "4749e584-2d31-4a0e-a104-0eb1d93c34a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://cli.github.com/packages stable InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "57 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "pciutils is already the newest version (1:3.7.0-6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 57 not upgraded.\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m No NVIDIA/AMD GPU detected. Ollama will run in CPU-only mode.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "‚úÖ ollama installed successfully\n"
          ]
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!sudo apt-get install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "print(\"‚úÖ ollama installed successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E87dItJOV1wq"
      },
      "source": [
        "# üöÄ Step 2: Start Ollama Server\n",
        "\n",
        "**Why do we need this?**\n",
        "- Ollama must be running before we can generate embeddings\n",
        "- This starts a background server on port 11434\n",
        "- Server stays active until you restart the runtime\n",
        "\n",
        "**What happens:**\n",
        "- Server starts in background\n",
        "- Waits 5 seconds to initialize\n",
        "- Ready to generate embeddings\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9tJr3CZb422",
        "outputId": "deacc9c1-9b44-4466-ab35-f9a269041bf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ ollama server starting...\n",
            "‚úÖ ollama server is running on port 11434\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import time\n",
        "\n",
        "def start_ollama_server():\n",
        "    subprocess.Popen(['ollama', 'serve'],\n",
        "                     stdout=subprocess.DEVNULL,\n",
        "                     stderr=subprocess.DEVNULL)\n",
        "    print(\"üîÑ ollama server starting...\")\n",
        "    time.sleep(5)\n",
        "    print(\"‚úÖ ollama server is running on port 11434\")\n",
        "\n",
        "start_ollama_server()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hooWFlBuV9Y7"
      },
      "source": [
        "# üì• Step 3: Download Embedding Model\n",
        "\n",
        "**What is nomic-embed-text?**\n",
        "- 768-dimensional embedding model\n",
        "- 274MB size\n",
        "- Optimized for semantic search\n",
        "- Converts text ‚Üí numbers\n",
        "\n",
        "**Why embeddings?**\n",
        "- Enables similarity search\n",
        "- \"machine learning\" and \"ML\" have similar embeddings\n",
        "- Math enables fast semantic search\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rtYvh-wPcEUn",
        "outputId": "05f89aa5-d6a7-4911-bcaf-5ef8a488d5a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "embedding model downloaded\n"
          ]
        }
      ],
      "source": [
        "# download the nomic-embed-text model for generating embeddings\n",
        "# this is a 274mb model optimized for semantic search\n",
        "# only needs to download once then its cached locally\n",
        "!ollama pull nomic-embed-text\n",
        "\n",
        "print(\"embedding model downloaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLDFpsm0WDHO"
      },
      "source": [
        "# üì¶ Step 4: Install Python Packages\n",
        "\n",
        "**What we're installing:**\n",
        "- **LangChain:** Framework for building RAG systems\n",
        "- **langchain-groq:** Fast LLM inference via Groq API\n",
        "- **ChromaDB:** Vector database for storing embeddings\n",
        "- **PyPDF2:** Extract text from PDF files\n",
        "- **Streamlit:** Web interface for the chatbot\n",
        "- **pyngrok:** Create public URL for Colab\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fF7iblqu9Xd",
        "outputId": "71ee1fc4-3706-4f24-f1b0-b1860999bc6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ all packages installed successfully\n"
          ]
        }
      ],
      "source": [
        "# install all the packages needed for the rag system\n",
        "# langchain is the main framework for building llm applications\n",
        "# langchain-community has integrations like ollama and chroma\n",
        "# langchain-groq lets us use groq api for fast llm inference\n",
        "# langchain-text-splitters handles chunking documents\n",
        "# langchain-chroma and chromadb are for vector storage\n",
        "# pypdf2 extracts text from pdf files\n",
        "!pip install -qU \\\n",
        "    langchain \\\n",
        "    langchain-community \\\n",
        "    langchain-groq \\\n",
        "    langchain-text-splitters \\\n",
        "    langchain-chroma \\\n",
        "    chromadb \\\n",
        "    PyPDF2 \\\n",
        "    streamlit \\\n",
        "    pyngrok\n",
        "\n",
        "print(\"‚úÖ all packages installed successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAUWe3M3WKnp"
      },
      "source": [
        "# üìö Step 5: Import Libraries\n",
        "\n",
        "**What we're importing:**\n",
        "- Embedding and LLM classes\n",
        "- Vector database tools\n",
        "- Text processing utilities\n",
        "- PDF readers\n",
        "\n",
        "**If this fails:** Previous package installation had an error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-b3tCJvrz8a",
        "outputId": "b65b74a9-3396-4b6b-be6b-7ac8141387a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from PyPDF2 import PdfReader\n",
        "from google.colab import userdata, drive\n",
        "\n",
        "print(\"‚úÖ libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NUa2XqmWRkM"
      },
      "source": [
        "# üîë Step 6: Test Groq API Key\n",
        "\n",
        "**What is Groq?**\n",
        "- Ultra-fast LLM inference service\n",
        "- 500+ tokens/second speed\n",
        "- Free tier: 14,400 requests/day\n",
        "\n",
        "**This cell verifies:**\n",
        "- Your API key is valid\n",
        "- Can connect to Groq servers\n",
        "- LLM responds correctly\n",
        "\n",
        "**If this fails:**\n",
        "1. Get key from console.groq.com/keys\n",
        "2. Add to Colab Secrets as `GROQ_API_KEY`\n",
        "3. Make sure it starts with `gsk_`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnPazbstu1tJ",
        "outputId": "82ce6115-5885-41a8-87cd-daef7306842c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç checking your groq api key:\n",
            "   length: 56 characters\n",
            "   preview: gsk_FpWrcH...Spkgiq\n",
            "‚úÖ groq test successful! response: Hello\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "groq_key = userdata.get('GROQ_API_KEY').strip()\n",
        "\n",
        "print(\"üîç checking your groq api key:\")\n",
        "print(f\"   length: {len(groq_key)} characters\")\n",
        "print(f\"   preview: {groq_key[:10]}...{groq_key[-6:]}\")\n",
        "\n",
        "llm = ChatGroq(\n",
        "    groq_api_key=groq_key,\n",
        "    model_name=\"llama-3.3-70b-versatile\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "try:\n",
        "    response = llm.invoke(\"say hello in one word\")\n",
        "    print(f\"‚úÖ groq test successful! response: {response.content}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå error with groq: {str(e)}\")\n",
        "    print(\"\\nif this fails check:\")\n",
        "    print(\"   1. copied entire key from console.groq.com/keys\")\n",
        "    print(\"   2. key starts with gsk_\")\n",
        "    print(\"   3. added to colab secrets as GROQ_API_KEY\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81J_2CbNWYrc"
      },
      "source": [
        "# ‚öôÔ∏è Step 7: Initialize Both Models\n",
        "\n",
        "**What this does:**\n",
        "- Sets up Ollama for embeddings (local)\n",
        "- Sets up Groq for LLM responses (cloud)\n",
        "- Tests both to make sure they work\n",
        "\n",
        "**Quick test:**\n",
        "- Generates a test embedding (768 numbers)\n",
        "- Gets a test response from Groq\n",
        "\n",
        "**You'll see:**\n",
        "- Embedding dimension: 768\n",
        "- Groq response: \"ok\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQavxcjLvQrn",
        "outputId": "f527e42d-a0f3-4001-ed97-3205196854f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ollama and groq initialized\n",
            "\n",
            "üî¢ ollama test: generated 768-dimensional embedding\n",
            "üìä first 10 values: [-0.6045737266540527, 0.39139172434806824, -3.717906951904297, -0.23124998807907104, 0.6087762117385864, 0.7964234948158264, -0.8758982419967651, 0.7451996803283691, -0.7282124161720276, 0.14450234174728394]\n",
            "\n",
            "üí¨ groq test: ok\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "\n",
        "# setup ollama embeddings\n",
        "embedding = OllamaEmbeddings(\n",
        "    base_url=\"http://localhost:11434\",\n",
        "    model=\"nomic-embed-text\"\n",
        ")\n",
        "\n",
        "# setup groq llm\n",
        "groq_key = userdata.get('GROQ_API_KEY').strip()\n",
        "llm = ChatGroq(\n",
        "    groq_api_key=groq_key,\n",
        "    model_name=\"llama-3.3-70b-versatile\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "print(\"‚úÖ ollama and groq initialized\\n\")\n",
        "\n",
        "# test ollama embeddings\n",
        "test_vec = embedding.embed_query(\"harrypotter\")\n",
        "print(f\"üî¢ ollama test: generated {len(test_vec)}-dimensional embedding\")\n",
        "print(f\"üìä first 10 values: {test_vec[:10]}\")\n",
        "\n",
        "# test groq llm\n",
        "test_response = llm.invoke(\"say ok\")\n",
        "print(f\"\\nüí¨ groq test: {test_response.content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOS7reHzW3Re"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V84i_6DaW-P5"
      },
      "source": [
        "# üìÑ Step 8: Test PDF Processing with Visual Outputs\n",
        "\n",
        "**What RAG needs:**\n",
        "1. **Extract text** from PDF\n",
        "2. **Split into chunks** (800 chars each with 200 overlap)\n",
        "3. **Generate embeddings** for each chunk\n",
        "4. **Store in vector database**\n",
        "\n",
        "**Why chunking?**\n",
        "- LLMs have token limits\n",
        "- Smaller chunks = more precise retrieval\n",
        "- Overlap preserves context across boundaries\n",
        "\n",
        "**This cell shows you:**\n",
        "- How many chunks were created\n",
        "- Example chunks from your PDF\n",
        "- Example embeddings (the actual numbers!)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpxydD1bygAR",
        "outputId": "e9b247b0-e3f1-44a2-fa1e-bd0361e17293"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ database directory ready\n",
            "\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ extracted 24,497 characters from 12 pages\n",
            "\n",
            "üìñ preview of extracted text:\n",
            "06112022 /V5  Tentaive scheme for Computer Science and Engineering and allied branches (CSE/ISE and BT) \n",
            "1 \n",
            " Visvesvaraya Technological University, Belagavi \n",
            "Scheme of Teaching and Examinations- 2022  \n",
            "Outcome-Based Education(OBE)and Choice Based Credit System(CBCS) \n",
            "(Effective   from the academic y...\n",
            "\n",
            "‚úÇÔ∏è split into 41 chunks\n",
            "\n",
            "======================================================================\n",
            "üì¶ EXAMPLE CHUNKS (showing first 3):\n",
            "======================================================================\n",
            "\n",
            "--- Chunk 1 (778 chars) ---\n",
            "06112022 /V5  Tentaive scheme for Computer Science and Engineering and allied branches (CSE/ISE and BT) \n",
            "1 \n",
            " Visvesvaraya Technological University, Belagavi \n",
            "Scheme of Teaching and Examinations- 2022  \n",
            "Outcome-Based Education(OBE)and Choice Based Credit System(CBCS) \n",
            "(Effective   from the academic year 2022- 23) \n",
            "I Semester   (CSE    Streams)                                                                                                         (Physics Group) \n",
            " \n",
            " \n",
            "Sl. \n",
            "No  \n",
            " \n",
            "Course \n",
            "andCourseCode  \n",
            " \n",
            " \n",
            "Course Title \n",
            "TD/PSB Teaching \n",
            "Hours/Week Examination \n",
            " \n",
            "Credits Theory \n",
            "Lecture \n",
            "Tutorial \n",
            "Practical/\n",
            "Drawing \n",
            "SDA \n",
            "Duration in \n",
            "hours \n",
            " \n",
            "CIE \n",
            "Marks \n",
            " \n",
            "SEE \n",
            "Marks \n",
            "Total \n",
            "Marks \n",
            "L T P S \n",
            "1 *ASC(IC) 22MATS 11 Mathematics for CSE Stream-I Maths 2 2 2 0 03 50 50 100  04\n",
            "\n",
            "\n",
            "--- Chunk 2 (792 chars) ---\n",
            "Lecture \n",
            "Tutorial \n",
            "Practical/\n",
            "Drawing \n",
            "SDA \n",
            "Duration in \n",
            "hours \n",
            " \n",
            "CIE \n",
            "Marks \n",
            " \n",
            "SEE \n",
            "Marks \n",
            "Total \n",
            "Marks \n",
            "L T P S \n",
            "1 *ASC(IC) 22MATS 11 Mathematics for CSE Stream-I Maths 2 2 2 0 03 50 50 100  04 \n",
            "2 #ASC(IC) 22PHYS12 Physics for CSE stream  Physics 2 2 2 0 03+02  50 50 100  04 \n",
            "3 ESC 22POP13 Principles of Programming Using C CSE 2 0 2 0 03+02  50 50 100  03 \n",
            "4 ESC-I 22ESC14x Engineering Science Course-I Respective Engg \n",
            "Dept 3 0 0 0 03 50 50 100  03 \n",
            "5 ETC- I/ 22ETC15x Emerging Technology Course- I  \n",
            "Any Engg Dept 3 0 0 0 03 \n",
            "50 50 100  03 OR      \n",
            "PLC-I 22PLC15x Programming Languages Course-I 2 0 2 0 03+02  \n",
            "6 AEC  22ENG16 Communicative English Humanities 0 2 0 0 01 50 50 100  01 \n",
            "7 HSMC 22KSK17 \n",
            "22KBK17 Samskrutika Kannada/ Balake \n",
            "Kannada Humanities 0 2 0 0 01 50 50 100  01 \n",
            " OR\n",
            "\n",
            "\n",
            "--- Chunk 3 (693 chars) ---\n",
            "6 AEC  22ENG16 Communicative English Humanities 0 2 0 0 01 50 50 100  01 \n",
            "7 HSMC 22KSK17 \n",
            "22KBK17 Samskrutika Kannada/ Balake \n",
            "Kannada Humanities 0 2 0 0 01 50 50 100  01 \n",
            " OR \n",
            "20ICO17 Indian Constitution \n",
            "8 AEC/SDC 22IDT18 Innovation and Design Thinking \n",
            "Any Dept 0 0 2 0 02 \n",
            "50 50 100  01  OR      \n",
            "22SFH18 Scientific Foundations of Health 1 0 0 0 01 \n",
            "TOTAL      400  400  800  20 \n",
            "SDA -Skill Development Activities,  TD/PSB - Teaching Department / Paper Setting Board, ASC -Applied Science Course, ESC - Engineering Science Courses, ETC - \n",
            "Emerging Technology Course, AEC - Ability Enhancement Course, HSMS -Humanity and Social Science and management Course, SDC - Skill Development Course,\n",
            "\n",
            "======================================================================\n",
            "üß† GENERATING EMBEDDINGS...\n",
            "======================================================================\n",
            "converting each chunk into 768-dimensional vector...\n",
            "this takes 1-2 minutes for multiple chunks\n",
            "\n",
            "‚úÖ created embeddings for all 41 chunks\n",
            "\n",
            "üîç let's look at one example embedding:\n",
            "   dimension: 768\n",
            "   first 20 values: [-0.16103936731815338, 0.3288435637950897, -2.208690881729126, -0.5743906497955322, 0.7648360133171082, 0.2200656682252884, 1.122517704963684, 0.03771350532770157, -0.024768365547060966, -0.11758873611688614, 0.1446925550699234, 0.6485291123390198, 1.291485071182251, 0.12881779670715332, -0.5225893259048462, -0.7112891674041748, 0.0158024150878191, -0.8892099261283875, -0.8979511260986328, -0.756038248538971]\n",
            "   these numbers represent the semantic meaning of the text!\n",
            "\n",
            "‚úÖ vector store ready with 41 documents\n",
            "\n",
            "üí° NOTE: this is in-memory storage (for testing only)\n",
            "   the streamlit app will save to disk properly\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_chroma import Chroma\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "db_path = \"/content/chroma_db\"\n",
        "if os.path.exists(db_path):\n",
        "    shutil.rmtree(db_path)\n",
        "print(\"‚úÖ database directory ready\\n\")\n",
        "\n",
        "# mount drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# load your pdf\n",
        "pdf_path = '/content/drive/MyDrive/csesch.pdf'  # CHANGE THIS PATH\n",
        "pdfreader = PdfReader(pdf_path)\n",
        "\n",
        "# extract text\n",
        "raw_text = ''\n",
        "for page in pdfreader.pages:\n",
        "    content = page.extract_text()\n",
        "    if content:\n",
        "        raw_text += content\n",
        "\n",
        "print(f\"‚úÖ extracted {len(raw_text):,} characters from {len(pdfreader.pages)} pages\")\n",
        "print(f\"\\nüìñ preview of extracted text:\")\n",
        "print(f\"{raw_text[:300]}...\\n\")\n",
        "\n",
        "# split into chunks\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\",\n",
        "    chunk_size=800,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "texts = text_splitter.split_text(raw_text)\n",
        "print(f\"‚úÇÔ∏è split into {len(texts)} chunks\\n\")\n",
        "\n",
        "# show example chunks\n",
        "print(\"=\"*70)\n",
        "print(\"üì¶ EXAMPLE CHUNKS (showing first 3):\")\n",
        "print(\"=\"*70)\n",
        "for i, chunk in enumerate(texts[:3], 1):\n",
        "    print(f\"\\n--- Chunk {i} ({len(chunk)} chars) ---\")\n",
        "    print(chunk)\n",
        "    print()\n",
        "\n",
        "# generate embeddings WITHOUT PERSISTENCE (for testing)\n",
        "print(\"=\"*70)\n",
        "print(\"üß† GENERATING EMBEDDINGS...\")\n",
        "print(\"=\"*70)\n",
        "print(\"converting each chunk into 768-dimensional vector...\")\n",
        "print(\"this takes 1-2 minutes for multiple chunks\\n\")\n",
        "\n",
        "# CREATE WITHOUT PERSIST DIRECTORY (in-memory only)\n",
        "vectorstore = Chroma.from_texts(\n",
        "    texts=texts,\n",
        "    embedding=embedding\n",
        "    # NO persist_directory for testing!\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ created embeddings for all {len(texts)} chunks\")\n",
        "print(f\"\\nüîç let's look at one example embedding:\")\n",
        "example_embedding = embedding.embed_query(texts[0])\n",
        "print(f\"   dimension: {len(example_embedding)}\")\n",
        "print(f\"   first 20 values: {example_embedding[:20]}\")\n",
        "print(f\"   these numbers represent the semantic meaning of the text!\")\n",
        "\n",
        "print(f\"\\n‚úÖ vector store ready with {len(texts)} documents\")\n",
        "print(\"\\nüí° NOTE: this is in-memory storage (for testing only)\")\n",
        "print(\"   the streamlit app will save to disk properly\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujcwmRO9zCCo",
        "outputId": "c608099b-ac27-40f6-97d1-d84b9d4af35a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rag query function ready\n"
          ]
        }
      ],
      "source": [
        "def ask_question(query, k=4):\n",
        "    \"\"\"\n",
        "    main rag function that retrieves context and generates answer\n",
        "    query: the user's question\n",
        "    k: number of most relevant chunks to retrieve (default 4)\n",
        "    \"\"\"\n",
        "\n",
        "    # step 1: search vector database for most similar chunks to the query\n",
        "    # uses cosine similarity between query embedding and stored embeddings\n",
        "    docs = vectorstore.similarity_search(query, k=k)\n",
        "\n",
        "    # step 2: combine all retrieved chunks into one context string\n",
        "    # separating with --- makes it clear where each chunk starts\n",
        "    context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "    # step 3: build the prompt for the llm\n",
        "    # we give it context first, then the question\n",
        "    # important: we tell it to only use the context provided\n",
        "    prompt = f\"\"\"answer the question based on the context below. be specific and cite information from the context. if you cannot answer based on the context, say you dont have enough information in the provided documents.\n",
        "\n",
        "context:\n",
        "{context}\n",
        "\n",
        "question: {query}\n",
        "\n",
        "answer:\"\"\"\n",
        "\n",
        "    # step 4: send prompt to groq and get response\n",
        "    response = llm.invoke(prompt)\n",
        "\n",
        "    # return both the answer and the source chunks\n",
        "    # sources help with transparency and debugging\n",
        "    return {\n",
        "        'answer': response.content,\n",
        "        'sources': docs\n",
        "    }\n",
        "\n",
        "print(\"rag query function ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "dxWgf9a3zKc7",
        "outputId": "c0186645-6b55-4203-a6fa-b38f010f24db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rag system ready, ask questions about your document\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5773460.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# keeps running until user types quit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"your question (or type quit to exit): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# check if user wants to quit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "print(\"rag system ready, ask questions about your document\\n\")\n",
        "\n",
        "# interactive question and answer loop\n",
        "# keeps running until user types quit\n",
        "while True:\n",
        "    query = input(\"your question (or type quit to exit): \").strip()\n",
        "\n",
        "    # check if user wants to quit\n",
        "    if query.lower() in ['quit', 'exit', 'q']:\n",
        "        print(\"goodbye\")\n",
        "        break\n",
        "\n",
        "    # skip empty inputs\n",
        "    if not query:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"question: {query}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    # get answer from rag system\n",
        "    result = ask_question(query)\n",
        "\n",
        "    print(f\"answer:\\n{result['answer']}\\n\")\n",
        "\n",
        "    # show which chunks were used to generate the answer\n",
        "    # helps with transparency and debugging\n",
        "    print(f\"sources used (top {len(result['sources'])} relevant chunks):\")\n",
        "    for i, doc in enumerate(result['sources'], 1):\n",
        "        print(f\"\\n   [{i}] {doc.page_content[:150]}...\")\n",
        "\n",
        "    print(f\"\\n{'='*60}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Ss7b5lVizMSM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0941a318-8b00-4ecc-feea-b9ea60f12c72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit installed!\n"
          ]
        }
      ],
      "source": [
        "# Install Streamlit and tunneling\n",
        "!pip install -q streamlit pyngrok\n",
        "\n",
        "\n",
        "print(\"Streamlit installed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G_5lZewXNlY"
      },
      "source": [
        "# üé® Step 9: Create Streamlit App\n",
        "\n",
        "**What is Streamlit?**\n",
        "- Web framework for Python\n",
        "- Creates chat interfaces easily\n",
        "- No HTML/CSS/JavaScript needed\n",
        "\n",
        "**This cell creates `app.py` with:**\n",
        "- Sidebar for configuration\n",
        "- Multiple PDF upload\n",
        "- Chat interface\n",
        "- Source citations\n",
        "\n",
        "**The app includes:**\n",
        "- Groq API key input\n",
        "- Drag-and-drop PDF upload\n",
        "- Process documents button\n",
        "- Chat history with sources\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "P7zcfP_Vz0YW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04e4330d-06ae-4c0b-d48f-65cb0d564828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from PyPDF2 import PdfReader\n",
        "import tempfile\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# setup the page configuration\n",
        "st.set_page_config(\n",
        "    page_title=\"rag chatbot\",\n",
        "    page_icon=\"ü§ñ\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# initialize session state variables\n",
        "if 'messages' not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "if 'vectorstore' not in st.session_state:\n",
        "    st.session_state.vectorstore = None\n",
        "if 'embeddings' not in st.session_state:\n",
        "    st.session_state.embeddings = None\n",
        "if 'llm' not in st.session_state:\n",
        "    st.session_state.llm = None\n",
        "if 'doc_count' not in st.session_state:\n",
        "    st.session_state.doc_count = 0\n",
        "\n",
        "# sidebar for configuration\n",
        "with st.sidebar:\n",
        "    st.title(\"configuration\")\n",
        "\n",
        "    groq_key = st.text_input(\"enter your groq api key\", type=\"password\", value=\"\")\n",
        "\n",
        "    if groq_key and not st.session_state.llm:\n",
        "        try:\n",
        "            st.session_state.embeddings = OllamaEmbeddings(\n",
        "                base_url=\"http://localhost:11434\",\n",
        "                model=\"nomic-embed-text\"\n",
        "            )\n",
        "\n",
        "            st.session_state.llm = ChatGroq(\n",
        "                groq_api_key=groq_key.strip(),\n",
        "                model_name=\"llama-3.3-70b-versatile\",\n",
        "                temperature=0\n",
        "            )\n",
        "            st.success(\"models initialized successfully\")\n",
        "        except Exception as e:\n",
        "            st.error(f\"error initializing models: {str(e)}\")\n",
        "\n",
        "    st.divider()\n",
        "\n",
        "    st.subheader(\"upload your documents\")\n",
        "    uploaded_files = st.file_uploader(\n",
        "        \"drag and drop multiple pdfs here\",\n",
        "        type=['pdf'],\n",
        "        accept_multiple_files=True\n",
        "    )\n",
        "\n",
        "    if uploaded_files and st.session_state.embeddings:\n",
        "        if st.button(\"process all documents\"):\n",
        "            with st.spinner(\"processing your documents...\"):\n",
        "                try:\n",
        "                    all_texts = []\n",
        "                    total_pages = 0\n",
        "                    total_chars = 0\n",
        "\n",
        "                    for uploaded_file in uploaded_files:\n",
        "                        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:\n",
        "                            tmp_file.write(uploaded_file.read())\n",
        "                            tmp_path = tmp_file.name\n",
        "\n",
        "                        pdfreader = PdfReader(tmp_path)\n",
        "                        raw_text = ''\n",
        "                        for page in pdfreader.pages:\n",
        "                            content = page.extract_text()\n",
        "                            if content:\n",
        "                                raw_text += content\n",
        "\n",
        "                        total_pages += len(pdfreader.pages)\n",
        "                        total_chars += len(raw_text)\n",
        "\n",
        "                        text_splitter = CharacterTextSplitter(\n",
        "                            separator=\"\\n\",\n",
        "                            chunk_size=800,\n",
        "                            chunk_overlap=200,\n",
        "                        )\n",
        "                        texts = text_splitter.split_text(raw_text)\n",
        "                        all_texts.extend(texts)\n",
        "\n",
        "                        os.unlink(tmp_path)\n",
        "\n",
        "                    # COMPLETE FRESH START - delete old db\n",
        "                    db_path = \"/content/chroma_db\"\n",
        "                    if os.path.exists(db_path):\n",
        "                        shutil.rmtree(db_path)\n",
        "\n",
        "                    # wait a moment\n",
        "                    import time\n",
        "                    time.sleep(1)\n",
        "\n",
        "                    # create fresh directory with full permissions\n",
        "                    os.makedirs(db_path, mode=0o777)\n",
        "\n",
        "                    # create vector store WITHOUT persist first (in-memory)\n",
        "                    # then save it\n",
        "                    st.session_state.vectorstore = Chroma.from_texts(\n",
        "                        texts=all_texts,\n",
        "                        embedding=st.session_state.embeddings\n",
        "                    )\n",
        "\n",
        "                    st.session_state.doc_count = len(uploaded_files)\n",
        "\n",
        "                    st.success(f\"processed {len(uploaded_files)} documents successfully\")\n",
        "                    st.info(f\"total: {total_pages} pages, {total_chars:,} characters, {len(all_texts)} chunks\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    st.error(f\"error processing pdfs: {str(e)}\")\n",
        "\n",
        "    if st.session_state.vectorstore:\n",
        "        st.success(f\"ready to answer questions from {st.session_state.doc_count} documents\")\n",
        "\n",
        "    st.divider()\n",
        "\n",
        "    if st.button(\"clear chat history\"):\n",
        "        st.session_state.messages = []\n",
        "        st.rerun()\n",
        "\n",
        "# main chat interface\n",
        "st.title(\"rag chatbot with ollama and groq\")\n",
        "st.caption(\"upload multiple pdfs and ask questions about them\")\n",
        "\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "        if \"sources\" in message:\n",
        "            with st.expander(\"view sources\"):\n",
        "                for i, source in enumerate(message[\"sources\"], 1):\n",
        "                    st.text(f\"[{i}] {source[:200]}...\")\n",
        "\n",
        "if prompt := st.chat_input(\"ask anything about your documents...\"):\n",
        "\n",
        "    if not st.session_state.vectorstore:\n",
        "        st.error(\"please upload and process at least one pdf first\")\n",
        "    elif not st.session_state.llm:\n",
        "        st.error(\"please enter your groq api key in the sidebar\")\n",
        "    else:\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.markdown(prompt)\n",
        "\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"thinking...\"):\n",
        "                try:\n",
        "                    docs = st.session_state.vectorstore.similarity_search(prompt, k=4)\n",
        "\n",
        "                    context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "                    full_prompt = f\"\"\"answer the question based only on the context below. be specific and cite information from the context. if you cannot answer based on the context, say you dont have enough information.\n",
        "\n",
        "context from documents:\n",
        "{context}\n",
        "\n",
        "user question: {prompt}\n",
        "\n",
        "answer:\"\"\"\n",
        "\n",
        "                    response = st.session_state.llm.invoke(full_prompt)\n",
        "                    answer = response.content\n",
        "\n",
        "                    st.markdown(answer)\n",
        "\n",
        "                    sources = [doc.page_content for doc in docs]\n",
        "                    with st.expander(\"view sources\"):\n",
        "                        for i, source in enumerate(sources, 1):\n",
        "                            st.text(f\"[{i}] {source[:200]}...\")\n",
        "\n",
        "                    st.session_state.messages.append({\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": answer,\n",
        "                        \"sources\": sources\n",
        "                    })\n",
        "\n",
        "                except Exception as e:\n",
        "                    error_msg = f\"sorry, encountered an error: {str(e)}\"\n",
        "                    st.error(error_msg)\n",
        "                    st.session_state.messages.append({\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": error_msg\n",
        "                    })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioxb2PYVXWyz"
      },
      "source": [
        "# üåê Step 10: Launch Streamlit with Ngrok\n",
        "\n",
        "**What is ngrok?**\n",
        "- Creates public URL for your Colab app\n",
        "- Anyone with the link can access it\n",
        "- Free tier available\n",
        "\n",
        "**What happens:**\n",
        "1. Starts Streamlit server on port 8501\n",
        "2. Creates ngrok tunnel\n",
        "3. Gives you a public URL\n",
        "\n",
        "**Share the URL with:**\n",
        "- Your juniors for the workshop\n",
        "- Anyone who wants to test the chatbot\n",
        "\n",
        "\n",
        "**To stop:** Runtime ‚Üí Interrupt execution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "8GjWpvMT1Tma",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9b92052-0be2-4c8d-f08d-f0dfc2c0cd39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting streamlit server...\n",
            "\n",
            "======================================================================\n",
            "your rag chatbot is live\n",
            "======================================================================\n",
            "\n",
            "public url: https://unblinking-gushily-starr.ngrok-free.dev\n",
            "\n",
            "instructions:\n",
            "   1. click the url above to open in new tab\n",
            "   2. enter your groq api key in the sidebar\n",
            "   3. drag and drop your pdf files\n",
            "   4. click process all documents\n",
            "   5. start asking questions\n",
            "\n",
            "to stop the app: runtime menu > interrupt execution\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "import time\n",
        "from google.colab import userdata\n",
        "\n",
        "# get ngrok token from colab secrets\n",
        "ngrok_token = userdata.get('NGROK_AUTH_TOKEN')\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "ngrok.kill()  # kill any existing tunnels first\n",
        "\n",
        "# start streamlit server in background\n",
        "# runs on port 8501 which is streamlits default\n",
        "process = subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE\n",
        ")\n",
        "\n",
        "print(\"starting streamlit server...\")\n",
        "time.sleep(8)  # give it time to fully start up\n",
        "\n",
        "# create public ngrok tunnel to the streamlit port\n",
        "public_url = ngrok.connect(8501, bind_tls=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"your rag chatbot is live\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\npublic url: {public_url.public_url}\")\n",
        "print(f\"\\ninstructions:\")\n",
        "print(\"   1. click the url above to open in new tab\")\n",
        "print(\"   2. enter your groq api key in the sidebar\")\n",
        "print(\"   3. drag and drop your pdf files\")\n",
        "print(\"   4. click process all documents\")\n",
        "print(\"   5. start asking questions\")\n",
        "print(f\"\\nto stop the app: runtime menu > interrupt execution\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PX03nBjvXfWi"
      },
      "source": [
        "# üîß Step 11: Troubleshooting Cell (Run Only If Errors)\n",
        "\n",
        "**When to run this:**\n",
        "- If you get \"connection refused\" errors\n",
        "- If processing documents fails\n",
        "- If ollama stops responding\n",
        "\n",
        "**What it does:**\n",
        "- Kills any stuck ollama processes\n",
        "- Restarts ollama server cleanly\n",
        "- Verifies it's running on port 11434\n",
        "\n",
        "**Don't run this unless you have problems!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "fO-TvENCXhXu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ef2e90c-039b-4196-f2e9-48c52e0b9292"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ restarting ollama server...\n",
            "‚úÖ ollama server is running successfully\n",
            "‚úÖ go back to streamlit and try again\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# kill existing ollama\n",
        "!pkill -9 ollama\n",
        "time.sleep(2)\n",
        "\n",
        "# restart ollama\n",
        "print(\"üîÑ restarting ollama server...\")\n",
        "subprocess.Popen(['ollama', 'serve'],\n",
        "                 stdout=subprocess.DEVNULL,\n",
        "                 stderr=subprocess.DEVNULL)\n",
        "time.sleep(10)\n",
        "\n",
        "# verify it's working\n",
        "try:\n",
        "    response = requests.get(\"http://localhost:11434/api/tags\", timeout=3)\n",
        "    if response.status_code == 200:\n",
        "        print(\"‚úÖ ollama server is running successfully\")\n",
        "        print(\"‚úÖ go back to streamlit and try again\")\n",
        "    else:\n",
        "        print(f\"‚ùå ollama responded with status: {response.status_code}\")\n",
        "except requests.exceptions.ConnectionError:\n",
        "    print(\"‚ùå ollama is not responding - try running cell 2 again\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "YfGjmZpR7UZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49bdcee9-15c4-4339-b303-e2b0416ba5f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root       36848  0.4  0.2 1782432 32296 ?       Sl   07:34   0:00 ollama serve\n",
            "root       36909  0.0  0.0   7376  3544 ?        S    07:34   0:00 /bin/bash -c ps aux | grep ollama\n",
            "root       36911  0.0  0.0   6484  2468 ?        S    07:34   0:00 grep ollama\n"
          ]
        }
      ],
      "source": [
        "# check if ollama process is running\n",
        "!ps aux | grep ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "AfrWdxlE_7Dz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad5f56eb-e9e8-4a5b-adb8-6359394c88fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tcp        0      0 127.0.0.1:11434         0.0.0.0:*               LISTEN     \n"
          ]
        }
      ],
      "source": [
        "# check if anything is listening on port 11434\n",
        "!netstat -tuln | grep 11434\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-hesiKq_-6q"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# kill any existing ollama\n",
        "!pkill -9 ollama\n",
        "time.sleep(2)\n",
        "\n",
        "# start ollama in background properly (using Popen not run)\n",
        "print(\"starting ollama server in background...\")\n",
        "subprocess.Popen(['ollama', 'serve'],\n",
        "                 stdout=subprocess.DEVNULL,\n",
        "                 stderr=subprocess.DEVNULL)\n",
        "\n",
        "# wait for it to initialize\n",
        "time.sleep(10)\n",
        "\n",
        "# test if its working\n",
        "try:\n",
        "    response = requests.get(\"http://localhost:11434/api/tags\", timeout=3)\n",
        "    if response.status_code == 200:\n",
        "        print(\"‚úì ollama server is running successfully on port 11434\")\n",
        "        print(\"‚úì go back to streamlit and click process all documents\")\n",
        "    else:\n",
        "        print(f\"√ó ollama responded but with status: {response.status_code}\")\n",
        "except requests.exceptions.ConnectionError:\n",
        "    print(\"√ó ollama is not responding on port 11434\")\n",
        "    print(\"√ó try running: !ollama --version to check if its installed\")\n",
        "except Exception as e:\n",
        "    print(f\"√ó error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUr4MnlKADUh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# completely remove old database\n",
        "!rm -rf /tmp/chroma_db\n",
        "\n",
        "# create fresh directory with full permissions\n",
        "os.makedirs(\"/tmp/chroma_db\", mode=0o777, exist_ok=True)\n",
        "\n",
        "# verify it was created\n",
        "if os.path.exists(\"/tmp/chroma_db\"):\n",
        "    print(\"‚úì database directory created successfully\")\n",
        "    print(f\"‚úì permissions: {oct(os.stat('/tmp/chroma_db').st_mode)[-3:]}\")\n",
        "else:\n",
        "    print(\"‚úó failed to create directory\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfhROGsvMaIF"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# delete old locked database\n",
        "!rm -rf /tmp/chroma_db\n",
        "\n",
        "# create completely fresh one\n",
        "os.makedirs(\"/tmp/chroma_db\", exist_ok=True)\n",
        "os.chmod(\"/tmp/chroma_db\", 0o777)  # full permissions\n",
        "\n",
        "print(\"fresh database created with full write permissions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qtqHmLIPzOw"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/chroma_db\n",
        "!chmod 777 /content/chroma_db\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvB2VfzLR0pF"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# kill any stuck ollama\n",
        "!pkill -9 ollama\n",
        "time.sleep(2)\n",
        "\n",
        "# restart ollama server\n",
        "print(\"üîÑ restarting ollama server...\")\n",
        "subprocess.Popen(['ollama', 'serve'],\n",
        "                 stdout=subprocess.DEVNULL,\n",
        "                 stderr=subprocess.DEVNULL)\n",
        "time.sleep(10)\n",
        "\n",
        "# verify it's running\n",
        "try:\n",
        "    response = requests.get(\"http://localhost:11434/api/tags\", timeout=3)\n",
        "    if response.status_code == 200:\n",
        "        print(\"‚úÖ ollama server is running successfully on port 11434\")\n",
        "        print(\"‚úÖ go back to streamlit and click 'process all documents' again\")\n",
        "    else:\n",
        "        print(f\"‚ùå ollama responded with status: {response.status_code}\")\n",
        "except requests.exceptions.ConnectionError:\n",
        "    print(\"‚ùå ollama is not responding - try running cell 2 again\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bat75qSfdZQM"
      },
      "outputs": [],
      "source": [
        "# FIX: Clean up and recreate database directory\n",
        "\n",
        "import os\n",
        "\n",
        "import shutil\n",
        "\n",
        "\n",
        "\n",
        "db_path = \"/content/chroma_db\"\n",
        "\n",
        "\n",
        "\n",
        "# Remove old database completely\n",
        "\n",
        "if os.path.exists(db_path):\n",
        "\n",
        "    shutil.rmtree(db_path)\n",
        "\n",
        "    print(\"‚úÖ Removed old database\")\n",
        "\n",
        "\n",
        "\n",
        "# Create fresh directory with full permissions\n",
        "\n",
        "os.makedirs(db_path, mode=0o777)\n",
        "\n",
        "print(\"‚úÖ Created fresh database directory\")\n",
        "\n",
        "\n",
        "\n",
        "# Verify permissions\n",
        "\n",
        "import stat\n",
        "\n",
        "perms = oct(os.stat(db_path).st_mode)[-3:]\n",
        "\n",
        "print(f\"‚úÖ Permissions: {perms} (should be 777)\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}